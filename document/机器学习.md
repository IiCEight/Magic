# Machine Learning



## 矩阵微分

约定加粗小写字母为列向量 $\textbf x$ , 大写字母为矩阵 $ X$, 小写字母为标量 $x$

### 实值函数

约定 $f$ 是一个实值函数，即 $f$ 的值域是 $R$, 若自变量是标量 $x$ 时， $f : R \rarr R$ ，若自变量是$n$维向量时 $\textbf x = (x_1, x_2, ..., x_n)^T$ ，相当与$f$ 是$n$元函数，$f(\textbf x) = f(x_1, x_2, ...,x_n)$，即 $f: R_n \rarr R$

此时定义标量函数对$n$维向量的偏导数为：
$$
\frac{\partial f(\textbf x)}{\partial \textbf x} = \frac{\partial f(x_1, x_2, ...,x_n)}{\partial \textbf x} = 
\left( 
\begin{array}{c}
\frac{\partial f(x_1, x_2, ...,x_n)}{\partial x_1}\\
\frac{\partial f(x_1, x_2, ...,x_n)}{\partial x_2}\\ 
...\\ 
\frac{\partial f(x_1, x_2, ...,x_n)}{\partial x_n}\\
\end{array}
\right)
$$
结果是一个列向量，每一维的值分别是该维标量对函数的偏导数。**本质就是多元函数偏导数的一个简写形式。**



### 向量值函数

若 $\textbf f$ 是一个向量值函数，即 $\textbf f$  的值域是$m$维向量, 若自变量是标量$x$时 $\textbf f : R \rarr R_m$ , e.g. 
$$
if \ f_i(x) = x^i, then\\

\textbf f(x) =
\begin{pmatrix}
f_1(x)\\ f_2(x)\\ f_3(x)\\ ...\\f_m(x)\\
\end{pmatrix}
=
\begin{pmatrix}
x\\ x^2\\ x^3\\ ...\\x^m\\
\end{pmatrix}
$$
 同理若自变量时$n$维向量时 $\textbf f : R_n \rarr R_m$
$$
\textbf f(\textbf x) =
\begin{pmatrix}
f_1(\textbf x)\\ f_2(\textbf x)\\ f_3(\textbf x)\\ ...\\f_m(\textbf x)\\
\end{pmatrix}
=
\begin{pmatrix}
f_1(x_1, x_2, ...,x_n)\\ f_2(x_1, x_2, ...,x_n)\\ f_3(x_1, x_2, ...,x_n)\\ ...\\
f_m(x_1, x_2, ...,x_n)\\
\end{pmatrix}
$$
此时定义向量函数对$n$维向量的偏导数为
$$
\frac{\partial \textbf  f(\textbf x)}{\partial \textbf x} =

\begin{pmatrix}
\frac{\partial \textbf f(\textbf x)}{\partial x_1}\\
\frac{\partial \textbf f(\textbf x)}{\partial x_2}\\ 
...\\ 
\frac{\partial \textbf f(\textbf x)}{\partial x_n}\\
\end{pmatrix} 
=

\begin{pmatrix}
\frac{\partial f_1(\textbf x)}{\partial x_1} & \frac{\partial f_2(\textbf x)}{\partial x_1} &
\frac{\partial f_3(\textbf x)}{\partial x_1} &... \frac{\partial f_m(\textbf x)}{\partial x_1}\\
\frac{\partial f_1(\textbf x)}{\partial x_2} & \frac{\partial f_2(\textbf x)}{\partial x_2} &
\frac{\partial f_3(\textbf x)}{\partial x_2} &... \frac{\partial f_m(\textbf x)}{\partial x_2}\\
...\\ 
\frac{\partial f_1(\textbf x)}{\partial x_n} & \frac{\partial f_2(\textbf x)}{\partial x_n} &
\frac{\partial f_3(\textbf x)}{\partial x_n} &... \frac{\partial f_m(\textbf x)}{\partial x_n}\\
\end{pmatrix}
$$
**本质就是对 $m$ 个实值函数和 $n$ 个变量的不同组合之间的求导**



### 常见矩阵微分公式

#### 1. $\frac{\partial A\textbf x}{\partial \textbf x} = \frac{\partial \textbf x^T A^T}{\partial \textbf x} = A^T$



设 $A_{m \times n}$ 是矩阵，$\textbf x = (x_1, x_2, ...,x_n)^T$ 是 $n$ 维未知数列向量, 则下述 $\textbf f$ 是一个向量值函数
$$
\textbf f(\textbf x) = A\textbf x =
\begin{pmatrix}
f_1(\textbf x)\\ f_2(\textbf x)\\ f_3(\textbf x)\\ ...\\f_m(\textbf x)\\
\end{pmatrix} =
\begin{pmatrix}
a_{11}x_1 + a_{12}x_2 + ... + a_{1n}x_n\\
a_{21}x_1 + a_{22}x_2 + ... + a_{2n}x_n\\ 
...\\ 
a_{m1}x_1 + a_{m2}x_2 + ... + a_{mn}x_n\\
\end{pmatrix} = x^TA^T
$$
其对 $\textbf x$ 的偏导数为
$$
\frac{\partial \textbf  f(\textbf x)}{\partial \textbf x} = \frac{\partial A\textbf x}{\partial \textbf x} = 

\begin{pmatrix}
\frac{\partial f_1(\textbf x)}{\partial x_1} & \frac{\partial f_2(\textbf x)}{\partial x_1} &
\frac{\partial f_3(\textbf x)}{\partial x_1} &... \frac{\partial f_m(\textbf x)}{\partial x_1}\\
\frac{\partial f_1(\textbf x)}{\partial x_2} & \frac{\partial f_2(\textbf x)}{\partial x_2} &
\frac{\partial f_3(\textbf x)}{\partial x_2} &... \frac{\partial f_m(\textbf x)}{\partial x_2}\\
...\\ 
\frac{\partial f_1(\textbf x)}{\partial x_n} & \frac{\partial f_2(\textbf x)}{\partial x_n} &
\frac{\partial f_3(\textbf x)}{\partial x_n} &... \frac{\partial f_m(\textbf x)}{\partial x_n}\\
\end{pmatrix} =\\
\begin{pmatrix}
a_{11} & a_{21}&  ... & a_{m1}\\
a_{12} & a_{22}&  ... & a_{m2}\\ 
...\\ 
a_{1n} & a_{2n}&  ... & a_{mn}\\
\end{pmatrix} = A^T
$$


#### 2. $\frac{\partial\textbf x^T A\textbf x}{\partial \textbf x} = (A + A^T)\textbf x$



设$A_{n \times n}$ 是方阵， $\textbf x = (x_1, x_2, ...,x_n)^T$, 是 $n$ 维未知数列向量, 则下述 $f$ 是一个实值函数（二次型）
$$
f(\textbf x) = \textbf x^T A\textbf x = 
a_{11}x_1^2 + (a_{12} + a_{21})x_1x_2 + (a_{13} + a_{31})x_1x_3+...+(a_{1n}+a_{n1})x_1x_n+\\
a_{22}x_2^2 + (a_{23} + a_{32})x_2x_3+...+(a_{21n}+a_{n2})x_2x_n+\\
...+\\
a_{nn}x_n^2\\\\

\frac{\partial f(\textbf x)}{\partial \textbf x} = \frac{\partial\textbf x^T A\textbf x}{\partial \textbf x}= \frac{\partial f(x_1, x_2, ...,x_n)}{\partial \textbf x} = 
\begin{pmatrix}
\frac{\partial f(x_1, x_2, ...,x_n)}{\partial x_1}\\
\frac{\partial f(x_1, x_2, ...,x_n)}{\partial x_2}\\ 
...\\ 
\frac{\partial f(x_1, x_2, ...,x_n)}{\partial x_n}\\
\end{pmatrix} =\\

\begin{pmatrix}
2a_{11}x_1 + (a_{12} + a_{21})x_2 + ... + (a_{1n} + a_{n1})x_n\\
(a_{21} + a_{12})x_1 + 2a_{22}x_2 + ... + (a_{2n} + a_{n2})x_n\\ 
...\\ 
(a_{n1} + a_{n2})x_1 + (a_{n2} + a_{2n})x_2 + ... + 2 a_{nn}x_n\\
\end{pmatrix} = 
(A + A^T)\textbf x
$$

#### 3. $\frac{\partial ||\textbf x||^2}{\partial\textbf  x}  =2\textbf x$

$$
\frac{\partial ||\textbf x||^2}{\partial\textbf  x} = \frac{\partial\textbf  x^T\textbf x}{\partial \textbf x} =2\textbf x
$$





#### 3.梯度 $\nabla$

由上面可以简写梯度
$$
\nabla f(\textbf x) = \frac{\partial f(\textbf x)}{\partial \textbf x} = 
\begin{pmatrix}
\frac{\partial  f(\textbf x)}{\partial x_1}\\
\frac{\partial  f(\textbf x)}{\partial x_2}\\ 
...\\ 
\frac{\partial f(\textbf x)}{\partial x_n}\\
\end{pmatrix}
$$





## 1 基本知识

在监督学习中，将输入与输出所有可能取值的集合分别称为输入空间(input space)与输出空间(outputspace)。



每个具体的输入是一个实例(instance)，通常由特征向量(feature vector)表示。这时，所有特征向量存在的空间称为特征空间(featurespace)。



在监督学习中，将输入与输出看作是定义在输入(特征)空间与输出空间上的随机变量的取值。输入输出变量用大写字母表示，习惯上输入变量写作$X$，输出变量写作$Y$。



监督学习的目的在于学习一个由输入到输出的映射，这一映射由模型来表示。换句话说，学习的目的就在于找到最好的这样的模型。模型属于由输入空间到输出空间的映射的集合，这个集合就是**假设空间(hypothesis space)**。



### 期望损失

由于型的输入、输出$(X,Y)$是随机变量，遵循联合分布$f_{X,Y}(X,Y)$，所以损失函数的期望是
$$
R_{exp}(f) = E_P[L(Y, f(X))] = \int L(y, f(x))f_{X,Y}(x, y)dxdy
$$
这是**理论上**模型$f(X)$关于联合分布$P(X,Y)$的平均意义下的损失，称为**风险函数(risk function)**或**期望损失(expected loss)**。

学习的目标就是选择期望风险最小的模型。由于联合分布$P(X,Y)$是未知的，$R_{exp}(f)$不能直接计算。实际上，如果知道联合分布$P(X,Y)$，可以从联合分布直接求出条件概率分布$P(Y|X)$，也就不需要学习了。正因为不知道联合概率分布所以才需要进行学习。这样一来，一方面根据期望风险最小学习模型要用到联合分布，另一方面联合分布又是未知的，所以监督学习就成为一个病态问题(ill-formed problem)。



### 经验损失

模型f(X)关于训练数据集的平均损失称为**经验风险(empirical risk)**或**经验损失(empirical loss)**
$$
R_{emp}(f) = \frac1N\sum_{i= 1}^NL(y_i, f(x_i))
$$
**根据大数定律，当样本容量趋于无穷时，经验风险$R_{emp}(f)$趋于期望风险 $R_{exp}(f)$。**所以一个很自然的想法是用经验风险估计期望风险。



### 泛化误差上界











## 3 线性模型

### 3.1基本形式


$$
f(\textbf x) = \textbf w^T \textbf x + b  \ \ (1)
$$

### 3.2 线性回归

在一元的情况下，
$$
f(x) = wx +b
$$
我们要找到参数$w, b$， 使得均方误差最小，即
$$
(w^*, b^*) = \arg\min_{(w, b)}\sum_{i = 1}^m (f(x_i) - y_i)^2
$$
这是个二元函数，通过分别求偏导数可以解得最小值 （略）



推广到多元线性回归，令 $\mathbf {\hat w} = (\textbf w^T, b)^T$
$$
X = 
\begin{pmatrix}
 \textbf x_1^T & 1\\
  \textbf x_2^T & 1\\
  ...\\
   \textbf x_m^T & 1\\
\end{pmatrix}
\\\\
X\mathbf {\hat w} = 
\begin{pmatrix}
  \textbf w^T \textbf x_1 + b\\
  \textbf w^T \textbf x_2 + b\\
  ...\\
  \textbf w^T \textbf x_m + b\\
\end{pmatrix}
\\ \\
$$
所以均方误差式就是向量 $\textbf y - X\mathbf {\hat w}$ 的内积


$$
\mathbf {\hat w^*} = \arg\min_{\mathbf {\bar w}}\sum_{i = 1}^m(y_i - \textbf w^T \textbf x_i + b)^2 =\arg\min_{\mathbf {\bar w}}(\textbf y - X\mathbf {\hat w})^T(\textbf y - X\mathbf {\hat w})
$$


令$f(\mathbf {\hat w}) = (\textbf y - X\mathbf {\hat w})^T(\textbf y - X\mathbf {\hat w})$， 可知这是一个实值函数，我们对 $\mathbf {\hat w}$ 求偏导
$$
\frac {\partial f(\mathbf {\hat w})}{\mathbf {\partial \hat w}} = \frac {\partial (\textbf y - X\mathbf {\hat w})^T(\textbf y - X\mathbf {\hat w})}{\mathbf {\partial \hat w}} = \\\frac {\partial \mathbf y^T \mathbf y}{\mathbf {\partial \hat w}} - \frac {\partial \mathbf y^T X\mathbf {\hat w}}{\mathbf {\partial \hat w}} - \frac {\partial \mathbf {\hat w} X\mathbf y^T}{\mathbf {\partial \hat w}} + \frac {\partial \mathbf {\hat w} X^TX\mathbf{\hat w}}{\mathbf {\partial \hat w}} = 2X^T(X\mathbf {\hat w} - \textbf y)
$$
若 $X^TX$ 存在逆矩阵，则令偏导为零可解得
$$
\mathbf {\hat w^*} = (X^TX)^{-1}X^T\textbf y
$$



### 3.3 对数几率回归 (Logistic Regression)

用于解决二分类问题，使用均方误差构造的函数不是凸函数，所以采用最大似然估计来寻找参数
$$
z = \textbf w^T \textbf x + b\\
f(\textbf x) = y = \frac {1}{1 +e^{-z}} = \frac {1}{1 +e^{-(\textbf w^T \textbf x + b)}}
\\
\frac{1}{y} = 1 + e^{-(\textbf w^T \textbf x + b)}\\
\frac{1 - y}{y} = e^{-(\textbf w^T \textbf x + b)}\\
\ln\frac{1 - y}{y} = -(\textbf w^T \textbf x + b)\\
\ln\frac{y}{1-y} = \textbf w^T \textbf x + b
$$
因为$y = \frac {1}{1 +e^{-z}}$ 的值域是$(0, 1)$, 若将$y$视为样本必作为正例的可能性,则$1-y$ 是其反例可能性。

将$y$ 视为类后验概率估计 $p(y = 1 | \textbf x)$
$$
\ln\frac{p(y = 1 | \textbf x)}{p(y = 0 | \textbf x)} = \textbf w^T \textbf x + b\\
p(y = 1 | \textbf x) = \frac {e^{(\textbf w^T \textbf x + b)}}{1 + e^{(\textbf w^T \textbf x + b)}}
$$
可以用最大似然估计来估计$\textbf w, b$参数, 使得所有的样本出现的概率的乘积最大，通过简单的取对数恒等变换转化为加法后
$$
L(\textbf w, b) = \sum_{i = 1}^m\ln (y_i p(y = 1 | \textbf x_i) + (1- y_i)p(y = 0 | \textbf x_i)) \\
$$
注意这里我们有$m$个样本，对于每个样本我们知道$y_i$ 是 $1$ 还是 $0$, 所有对于每个样本，我们要应用对应的概率$p(y = 1 | \textbf x)\  or \ p(y = 0 | \textbf x)$, 所以公式$y_i p(y = 1 | \textbf x_i) + (1- y_i)p(y = 0 | \textbf x_i)$, 当$y_i = 1$时， 值为$p(y = 1 | \textbf x_i)$, 反之为$p(y = 0 | \textbf x_i)$。

带入概率可得
$$
L(\textbf w, b) = \sum_{i = 1}^m (y_i(\textbf w^T \textbf x_i + b) - \ln(1 + e^{\textbf w^T \textbf x_i + b}))
$$
这里是求最大值，对应的是梯度上升，一般常见的是梯度下降法使损失函数最小化，所以我们在这里加一个负号
$$
J(\textbf w, b) = -\frac{1}{m}L(\textbf w, b)
$$
是凸函数，可以用梯度下降法，牛顿法求其最优解。$x_{ij}$代表样本$\textbf x_i$的第$j$个分量
$$
\frac{\partial J}{\partial \textbf w} = \frac{1}{m}\sum_{i =1}^{m}(-y_i \textbf x_i + \frac{e^{\textbf w^T \textbf x_i + b}}{1 + e^{\textbf w^T \textbf x_i + b}} \textbf x_i) =\\

\frac{1}{m}\sum_{i =1}^{m}(f(\textbf x_i)-y_i) \textbf x_i\\

\frac{\partial J}{\partial  w_j} = \frac{1}{m}\sum_{i =1}^{m}(-y_i  x_{ij} + \frac{e^{\textbf w^T \textbf x_i + b}}{1 + e^{\textbf w^T \textbf x_i + b}}  x_{ij}) =\\
\frac{1}{m}\sum_{i =1}^{m}(f(\textbf x_i) - y_i)  x_{ij}\\

\frac{\partial J}{\partial  b} = \frac{1}{m}\sum_{i =1}^{m}(f(\textbf x_i) - y_i)
$$







### 3.4 线性判别分析



### 3.5 多分类学习

softmax

### 3.6 类别不平衡问题



## 4 决策树



### 4.2 划分选择

#### 4.2.1 信息增益

实际上，信息增益准则对可取值数目较多的属性有所偏好

#### 4.2.2 增益率

需注意的是，增益率准则对可取值数目较少的属性有所偏好

#### 4.2.3 基尼指数



### 4.3 剪枝

剪枝(pruning)是决策树学习算法对付“过拟合”的主要手段。在决策树学 习中，为了尽可能正确分类训练样本，结点划分过程将不断重复，有时会造成决 策树分支过多，这时就可能因训练样本学得“太好”了，以致于把训练集自身 的一些特点当作所有数据都具有的一般性质而导致过拟合。因此，**可通过主动去掉一些分支来降低过拟合的风险**。



### 4.4 连续与缺失值

需注意的是，与离散属性不同，若当前结点划分属性为连续属性，该属性还可作为其后代结点的划分属性。



## 5 神经网络

### 5.1 神经元模型

$$
y  = f(\sum_{i = 1}^mw_ix_i - \theta)
$$

其中$f$ 叫做激活函数



[Hornik et al. , 1989]证明，只需一个包含足够多神经元的隐层，多层前馈网络就能以任意精度逼近任意复杂度的连续函数





## 6  支持向量机 (Support Vector Machine)

解决二分类问题。

找到一个超平面 $\textbf w^T \textbf x+b = 0$ ，能分割数据，**同时使得离该平面最近的点的距离最大**。

多维空间下点$ \textbf x$到超平面的距离公式为
$$
d = \frac {|\textbf w^T \textbf x+b |}{||\textbf w||}
$$


对于一个超平面 $\textbf w^T \textbf x+b = 0$ , 对两边同时乘以非零常数，直线不变，所有可以找到合适的$\textbf w',  b'$, 使得最近的点的离平面的距离为$1$

经过转化，问题变为求
$$
\min_{\textbf w,  b} \frac12||\textbf w||^2 \ \ s.t. \ \ y_i(\textbf w^T \textbf x_i + b) \ge 1 \ \ (i = 1, ..., m)
$$


这是一个**凸二次优化问题**



使用拉格朗日乘数法
$$
L(\textbf w, b, \alpha_i) =\frac12||\textbf w||^2 + \sum_{i = 1}^m\alpha_i(1 - y_i(\textbf w^T \textbf x_i + b) )
$$
分别求偏导求解。
$$
\frac {\partial L}{\partial \textbf w} = \textbf w - \sum_{i=1}^m\alpha_iy_i\textbf x_i \\
\frac {\partial L}{\partial b} = -\sum_{i=1}^m\alpha_iy_i
$$
令上述偏导数等于零解得
$$
\textbf w = \sum_{i=1}^m\alpha_iy_i\textbf x_i \\
\sum_{i=1}^m\alpha_iy_i = 0
$$
带入原式得到拉格朗日对偶函数
$$
L= \frac12\textbf w^T\textbf w + \sum_{i = 1}^m\alpha_i(1 - y_i( \sum_{i=1}^m\alpha_iy_i\textbf x_i^T\textbf x_i + b) ) = \\
\frac12(\sum_{i=1}^m\alpha_iy_i\textbf x_i^T)(\sum_{i=1}^m\alpha_iy_i\textbf x_i) + \sum_{i = 1}^m\alpha_i(1 - y_i( \sum_{j=1}^m\alpha_jy_j\textbf x_j^T\textbf x_i + b) ) =\\
\frac12 \sum_{i = 1}^m\sum_{j = 1}^m\alpha_i\alpha_jy_iy_j\textbf x_i^T\textbf x_j +
\sum_{i = 1}^m\alpha_i - \sum_{i = 1}^m\alpha_iy_i\textbf x_i\sum_{i = j}^m\alpha_jy_j\textbf x_j^T - \sum_{i = 1}^m\alpha_iy_ib =\\
\sum_{i = 1}^m\alpha_i - \frac12 \sum_{i = 1}^m\sum_{j = 1}^m\alpha_i\alpha_jy_iy_j\textbf x_i^T\textbf x_j - b\sum_{i = 1}^m\alpha_iy_i = \\
\sum_{i = 1}^m\alpha_i - \frac12 \sum_{i = 1}^m\sum_{j = 1}^m\alpha_i\alpha_jy_iy_j\textbf x_i^T\textbf x_j
$$

$$
max \ L(\alpha)\\
s.t. \ \ \alpha \succeq 0 \\
\sum_{i=1}^m\alpha_iy_i = 0
$$





**线性可分训练数据集的最大间隔分离超平面是存在且唯一的。**



## 7 贝叶斯分类器

### 7.1 朴素贝叶斯

假设：条件概率是独立的

解决多分类问题，输出空间是$\mathcal Y = {c_1, ...,c_m}$
$$
\arg\max_{c_k}P(Y = c_k | X = x)
$$
想法很简单，求在当前样本$x$ 的前提下，$Y =c_k$ 概率最大的$c_k$ .
$$
P(Y = c_k | X = x) = 
\frac {P(Y = c_k, X = x)}{P(X = x)} = \\
\frac {P(X = x | Y = c_k) P(Y = c_k)}{P(X = x)} =\\
\frac { P(Y = c_k)\sum_{i = 1}^mP(X^{(i)} =x^{(i)} |Y = c_k)}{P(X = x)}
$$


第三个等式就是利用的独立性假设。

因为对于所有的$Y = c_k$上式分母都是相同的，所有问题变成
$$
\arg\max_{c_k} P(Y = c_k)\sum_{i = 1}^mP(X^{(i)} =x^{(i)} |Y = c_k)
$$

### 7.2 原理

朴素贝叶斯法将实例分到后验概率最大的类中。这等价于期望风险最小化。选择0-1损失函数，即分类正确为零，分类错误为一。

这时期望风险为
$$
R_{exp}(f) = E[L(Y, f(X))] =\\
E[E[L(Y, f(X)) | X]] = \\
$$

### To be solved



### 7.3 朴素贝叶斯法的参数估计

所以只要估计$P(Y = c_k)$ 和 $P(X^{(i)} =x^{(i)} |Y = c_k)$.

方法就是根据样本中出现的次数和样本总量的比值来估计。



#### 7.3.2 贝叶斯估计

用极大似然估计可能会出现所要估计的概率值为0的情况。

在算频数时，分子分母加上合适的数，这样就不会出现概率为0的情况

（拉普拉斯平滑）









## 8 提升方法 (Boosting)

### 8.1 AdaBoost

解决二分类问题，将多个弱学习器通过线性组合的方式形成强学习器。





## 9 聚类

### 9.1 基本的概念

n个样本，每个样本m个属性。构成一个$m\times n$的矩阵，其中每个列向量代表一个样本。



#### 9.1.1 相似度度量

**闵可夫斯基距离**

若向量$a,b \in R^m$
$$
d(a, b) = (\sum_{i = 1}^m |a_k- b_k|^p)^{\frac 1p}\ \ \ (p \ge 1)
$$
$p = 2$ 时称为欧式距离。

$p = 1$ 时称为曼哈顿距离。

$p = \infin$ 时称为切比雪夫距离。



**马哈拉诺比斯距离**



**相关系数**



**夹角余弦**



#### 9.1.2 类或簇

$T \in R$, 若set $G$ 中任意两个样本$a, b$ , 有
$$
d(a, b) \le T
$$
then we say $G$ is a cluster.



The **mean value** of the cluster or **center** of the cluster $\bar x_{G}$
$$
\bar x_G = \frac 1{n_G}\sum_{i = 1}^{n_G}x_i
$$
$n_G$ is the number of samples in $G$.



**diameter**
$$
D_G = \max_{a, b\in G}d(a, b)
$$


**散布矩阵 (scatter matrix)**

**协方差矩阵 (covariance matrix)**



The distance between cluster $p$ and $q$.

**最短距离或单连接(single linkage)**
$$
D(p, q) = \min_{a \in p, b \in q}d(a, b)
$$


**最长距离或完全连接 (complete linkage)**
$$
D(p, q) = \max_{a \in p, b \in q}d(a, b)
$$


### 9.2 层次聚类

Every sample only belongs to one cluster, so it's a hard clustering.



聚合聚类开始将每个样本各自分到一个类;之后将相距最近的两类合并，建立一个新的类，重复此操作直到满足停止条件；得到层次化的类别。



### 9.3 k均值聚类

k均值聚类是基于样本集合划分的聚类算法。k均值聚类将样本集合划分为k个子集，构成k个类,将n个样本分到k个类中，每个样本到其所属类的中心的距离最小。每个样本只能属于一个类,所以k均值聚类是硬聚类。



这是一个组合优化问题，$n$个样本分到$k$类，所有可能分法的数目是**第二类斯特林数$S(n, k)$**

求最优解是一个$np$难的问题。现实采用迭代的方法求解。



#### 9.3.1 算法

k均值聚类的算法是一个迭代的过程，每次迭代包括两个步骤。首先选择k个类的中心，将样本逐个指派到与其最近的中心的类中，得到一个聚类结果;然后更新每个类的样本的均值，作为类的新的中心;重复以上步骤，直到收敛为止。



复杂度是$O(mnk)$



## 10 k 邻近法(k-nearest neighbor, k-NN)



### 10.1 算法

k近邻算法简单、直观:给定一个训练数据集，对新的输入实例，在训练数据集中找到与该实例最邻近的k个实例，这k个实例的多数属于某个类，就把该输入实例分为这个类。



### 10.2 KD Tree



## 11 主成分分析

see **Deep Learning** page 48
