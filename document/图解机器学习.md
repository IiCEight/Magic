# 图解机器学习



## Chapter 2 学习模型

### 2.1 线性模型

普通线性模型只能表现线性的输入和输出
$$
f_{\theta}( x) = \theta x 
$$
因此，引入基函数向量，使得其能表示非线性输入输出
$$
f_{\theta}( x) = \sum_{i = 1}^b\theta_i\phi_i(x)= \theta^T \phi(x)
$$
比如，若$\phi_i(x) = x^i$, 则模型变成通过**多项式**来拟合结果
$$
f_{\theta}(x) = \sum_{i = 1}^b\theta_ix^i = f_{\theta}( x) = \theta_1x + \theta_2x^2 + ... +\theta_bx^b 
$$


同时把输入扩展成$d$维向量$\textbf x = (x_1, x_2, ..., x_d)^T$的形式有
$$
f_{\theta}(\textbf x) = \sum_{i = 1}^d\sum_{j =1}^b\theta_{ij}\phi_j(\textbf x_i)
$$



### 2.2 核模型

把基函数换成核函数，即是核模型，核函数是二元函数$K(x, y)$, 模型变为
$$
f_{\theta}(\textbf x) = \sum_{i = 1}^b\theta_iK(\textbf x, \textbf x_i)
$$
其中$K(\textbf x, \textbf x_i)$ 中的$\textbf x_i$是第$i$个输入样本， 所以基函数与训练样本毫不相干，但是核函数会依赖训练样本。

在众多核函数中，高斯核函数使用最广泛
$$
K(\textbf x, \textbf c) = exp(-\frac{|| x - c||^2}{2h^2})
$$
其中$||\textbf x||$ 表示向量的模，即$||\textbf x|| = \sqrt {\textbf x^T\textbf x}$



### 2.3 层级模型

非线性模型



## Chapter 3 最小二乘学习法

### 3.1 最小二乘学习法

$n$个样本进行带参数的学习